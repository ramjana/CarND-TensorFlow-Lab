{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensforflow as tf\n",
    "import tensorflow.examples.tutorial.mnist import input_data\n",
    "import tensorflow.contrib.layers import flatten\n",
    "import numpy as np\n",
    "\n",
    "#Load data sets from MNIST DATABASE\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\",reshape=False,one_hot=True)\n",
    "train_features,train_labels = mnist.train.images, mnist.train.labels\n",
    "validation_featurs,validation_labels   = mnist.validation.images, mnist.validation.labels\n",
    "test_features, test_labels = mnist.test.images,mnist.test.labels\n",
    "\n",
    "assert(len(train_features) == len(train_labels))\n",
    "assert(len(validation_features) == len(validation_labels))\n",
    "assert(len(test_features) == len(test_labels))\n",
    "\n",
    "print()\n",
    "print(\"Image Shape: {}\".format(train_features[0].shape))\n",
    "print()\n",
    "print(\"Training Set:   {} samples\".format(len(train_features)))\n",
    "print(\"Validation Set: {} samples\".format(len(validation_features)))\n",
    "print(\"Test Set:       {} samples\".format(len(test_features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize data\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "index_image = random.randint(0,len(train_features))\n",
    "image = train_features[index_image]\n",
    "\n",
    "plt.figure(figsize=(1,1))\n",
    "plt.imshow(image, cmap=\"gray\")\n",
    "print(train_labels[index_image])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model parameters\n",
    "\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 128\n",
    "mu=0\n",
    "sigma = 0.1\n",
    "learning_rate = 0.001\n",
    "\n",
    "#define 5-layer lenet CONV=>RELU=>MAXPOOL=>CONV=>RELU=>MAXPOOL=>FC1=>RELU=>FC2=>RELU=>FC3\n",
    "def LeNet(input):\n",
    "       \n",
    "    wc1 = tf.Variable(tf.truncated_normal(shape=[5,5,1,6],mu=mu, stddev=sigma))\n",
    "    bc1 = tf.Variable(tf.zeros(6))\n",
    "    \n",
    "    # conv layer 1 input=32,32,1 output = 28,28,6\n",
    "    conv1 = tf.nn.conv2d(input,wc1,strides=[1,2,2,1],padding=\"VALID\")\n",
    "    conv1 = tf.add(conv1,bc1)\n",
    "    \n",
    "    #activation input = 28,28,6 output = 14,14,6\n",
    "    conv1 = tf.nn.relu(conv1)\n",
    "    \n",
    "    #pooling\n",
    "    conv1 = tf.nn.maxpool(conv1,ksize=[1,2,2,1],strides=[1,2,2,1],padding=\"VALID\")\n",
    "    \n",
    "    #conv layer 2 input = 14,14,6 output = 10,10,16\n",
    "    wc2 = tf.Variable(tf.truncated_normal([5,5,6,16],mu=mu,stddev=sigma))\n",
    "    bc2 = tf.Variable(tf.zeros(16))\n",
    "    \n",
    "    conv2 = tf.nn.conv2d(conv1,wc2,strides=[1,2,2,1],padding=\"VALID\")\n",
    "    conv2 = tf.add(conv2,bc2)\n",
    "    \n",
    "    #activation\n",
    "    conv2 = tf.nn.relu(conv2)\n",
    "    \n",
    "    # max pooling input = 10,10,16 output = 5,5,16   \n",
    "    conv2 = tf.nn.maxpool(conv2,ksize=[1,2,2,1],strides=[1,2,2,1],padding='VALID')\n",
    "    \n",
    "    wd1 = tf.Variable(tf.truncated_normal([400,120],mu=mu,stddev=sigma))\n",
    "    bd1 = tf.Variable(tf.zeros(120))\n",
    "    \n",
    "    # fully connected layer\n",
    "    fc1 =  tf.reshape(conv2,[-1,wd1.get_shape().as_list()[0]])\n",
    "    fc1 =  tf.add(tf.matmul(fc1,wd1),bd1)\n",
    "    fc1 =  tf.nn.relu(fc1)\n",
    "    \n",
    "    wd2 = tf.Variable(tf.truncated_normal([120,84],mu=mu,stddev=sigma))\n",
    "    bd2 = tf.Variable(tf.zeros(84))\n",
    "\n",
    "    fc2 = tf.add(tf.matmul(fc1,wd2),bd2)\n",
    "    fc2 = tf.nn.relu(fc2)\n",
    "    \n",
    "    wd3 = tf.Variable(tf.truncated_normal([84,10],mu=mu,stddev=sigma))\n",
    "    bd3 = tf.Variable(tf.zeros(10))\n",
    "    \n",
    "    logits = tf.add(tf.matmul(fc2,wd3),bd3)\n",
    "    \n",
    "    return logits\n",
    "    \n",
    "\n",
    "#function to normalize input image\n",
    "def normalize_grayscale(image_data):    \n",
    "\n",
    "    features = []\n",
    "    for dataItem in image_data:\n",
    "        feature = 0.1 + ((dataItem - 0)*(0.8))/(255)\n",
    "        features.append(feature)\n",
    "    return np.array(features)\t\n",
    "\n",
    "\n",
    "#preprocess data sets\n",
    "#normalize data sets\n",
    "train_features = normalize_grayscale(train_features)\n",
    "validation_features = normalize_grayscale(validation_features)\n",
    "test_features       = normalize_grayscale(test_features)\n",
    "\n",
    "#resize input features of dimention 28x28x1 to 32x32x1\n",
    "#add 2 rows of '0' on either side of input (both width,height)\n",
    "train_features = np.padd(train_features,[[0,0],[2,2],[2,2],[0,0]], mode=\"CONSTANT\")\n",
    "validation_features = np.padd(validation_features,[[0,0],[2,2],[2,2],[0,0]],mode=\"CONSTANT\")\n",
    "test_features = np.padd(test_features,[[0,0],[2,2],[2,2],[0,0]],mode=\"CONSTANT\")\n",
    "\n",
    "#shuffle train data sets\n",
    "train_features, train_labels = shuffle(train_features, train_labels)\n",
    "\n",
    "x = tf.placeholder(tf.float32,[None,32,32,1])  #None = batch_size  \n",
    "y = tf.placeholder(tf.float32,[None,10])\n",
    "\n",
    "logits = LeNet(x)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax.cross_entropy_with_logits(logits=logits,labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "saver =  tf.train.Saver()\n",
    "\n",
    "#Validate model accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(fc2,1),tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "\n",
    "#evalute model procedures\n",
    "\n",
    "def evaluate(x_data,y_data):\n",
    "    \n",
    "    num_examples = len(x_data)\n",
    "    total_accuracy = 0\n",
    "    total_loss = 0\n",
    "    sess=tf.get_default_session()\n",
    "    for step in range(0,num_examples,BATCH_SIZE):\n",
    "        data_start=step\n",
    "        data_end  = step+BATCH_SIZE\n",
    "        batch_x, batch_y = x_data[data_start:data_end],y_data[data_start:data_end]\n",
    "        loss = sess.run(cost,feed_dict={x: batch_x, y: batch_y})\n",
    "        accu = sess.run(accuracy, feed_dict = {x: batch_x, y: batch_y})\n",
    "        total_accuracy += accu*BATCH_SIZE\n",
    "        total_loss     += loss*BATCH_SIZE\n",
    "    \n",
    "    return total_loss/num_examples, total_accuracy/num_examples\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session as sess:\n",
    "    sess.run(init)\n",
    "    steps_per_epoch = len(train_features)//BATCH_SIZE\n",
    "    num_examples = steps_per_epoch*BATCH_SIZE\n",
    "  \n",
    "    print(\"Training model...\")\n",
    "    print()\n",
    "    for epoch_iter in range(EPOCHS)    \n",
    "        for step in range(steps_per_epoch):\n",
    "            batch_start = step*BATCH_SIZE\n",
    "            batch_x, batch_y = train_features[batch_start:batch_start+BATCH_SIZE],train_labels[batch_start:batch_start+BATCH_SIZE]\n",
    "            sess.run(optimizer, feed_dict={x: batch_x, y:batch_y})\n",
    "         \n",
    "        #steps_per_epoch = mnist.validation.num_examples // BATCH_SIZE\n",
    "        #num_examples =  steps_per_epoch*BATCH_SIZE\n",
    "        #for step in range(steps_per_epoch):\n",
    "        #    batch_start = step*BATCH_SIZE\n",
    "        #    batch_x, batch_y = validation_features[batch_start:batch_start+BATCH_SIZE]\n",
    "        #    loss = sess.run(cost, feed_dict= {x:batch_x, y:batch_y})\n",
    "        #    acc  = sess.run(accuracy,feed_dict={x: batch_x, y: batch_y})\n",
    "        #    total_acc += (acc*batch_x.shape[0])\n",
    "        #    total_loss += (loss*batch_x.shape[0])\n",
    "        \n",
    "        total_loss, total_acc = evaluate(validation_features,validation_labels)\n",
    "        print(\"EPOCH {} ...\".format(epoch_iter+1))\n",
    "        print(\"Validation loss = {:.3f}\".format(total_loss))\n",
    "        print(\"Validation accuracy = {:.3f}\".format(total_acc))\n",
    "        print()\n",
    "\n",
    "        saver.save(sess, './lenet')\n",
    "        print('Model Saved')\n",
    "        \n",
    "    # Evaluate on the test data\n",
    "    #steps_per_epoch = mnist.test.num_examples//BATCH_SIZE\n",
    "    #num_examples = steps_per_epoch*BATCH_SIZE\n",
    "    #for step in range(steps_per_epoch):\n",
    "    #    batch_x, batch_y = mnist.test_next_batch(BATCH_SIZE)\n",
    "    #    test_loss = sess.run(cost,feed_dict={x: batch_x, y: batch_y})\n",
    "    #    test_acc  = sess.run(accuracy,feed_dict={x: batch_x, y: batch_y})\n",
    "    #    total_acc += test_acc*batch_x.shape[0]\n",
    "    #    total_loss += test_loss*batch_x.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test the model\n",
    "print(\"Testing Lenet.......\")\n",
    "\n",
    "#restore the model\n",
    "saver.restore(sess, tf.train.latest_checkpoint('.'))\n",
    "\n",
    "total_loss , total_acc = evaluate(test_features,test_labels)\n",
    "print(\"Test loss= {:.3f}\".format(total_loss))\n",
    "print(\"Test accuracy = {:.3f}\".format(total_acc))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
