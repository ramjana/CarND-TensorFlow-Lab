{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensforflow as tf\n",
    "import tensorflow.examples.tutorial.mnist import input_data\n",
    "import tensorflow.contrib.layers import flatten\n",
    "\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 128\n",
    "mu=0\n",
    "sigma = 0.1\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "def LeNet(input):\n",
    "\n",
    "    #normalize if input images are not X' = a +(X-Xmin)*(b-a)/(Xmax-Xmin)\n",
    "    \n",
    "    # input is of 2D dimension need to convert 4D (tensor)\n",
    "    input = tf.reshape(input,(-1,28,28,1))\n",
    "    # add 2 rows on either side of width,height to conver to 32x32x1 input\n",
    "    input = tf.pad(input,[[0,0],[2,2],[2,2],[0,0]],mode=\"CONSTANT\")\n",
    "    \n",
    "    wc1 = tf.Variable(tf.truncated_normal(shape=[5,5,1,6],mu=mu, stddev=sigma))\n",
    "    bc1 = tf.Variable(tf.zeros(6))\n",
    "    \n",
    "    # conv layer 1 input=32,32,1 output = 28,28,6\n",
    "    conv1 = tf.nn.conv2d(input,wc1,strides=[1,2,2,1],padding=\"VALID\")\n",
    "    conv1 = tf.add(conv1,bc1)\n",
    "    \n",
    "    #activation input = 28,28,6 output = 14,14,6\n",
    "    conv1 = tf.nn.relu(conv1)\n",
    "    \n",
    "    #pooling\n",
    "    conv1 = tf.nn.maxpool(conv1,ksize=[1,2,2,1],strides=[1,2,2,1],padding=\"VALID\")\n",
    "    \n",
    "    #conv layer 2 input = 14,14,6 output = 10,10,16\n",
    "    wc2 = tf.Variable(tf.truncated_normal([5,5,6,16],mu=mu,stddev=sigma))\n",
    "    bc2 = tf.Variable(tf.zeros(16))\n",
    "    \n",
    "    conv2 = tf.nn.conv2d(conv1,wc2,strides=[1,2,2,1],padding=\"VALID\")\n",
    "    conv2 = tf.add(conv2,bc2)\n",
    "    \n",
    "    #activation\n",
    "    conv2 = tf.nn.relu(conv2)\n",
    "    \n",
    "    # max pooling input = 10,10,16 output = 5,5,16   \n",
    "    conv2 = tf.nn.maxpool(conv2,ksize=[1,2,2,1],strides=[1,2,2,1],padding='VALID')\n",
    "    \n",
    "    wd1 = tf.Variable(tf.truncated_normal([400,120],mu=mu,stddev=sigma))\n",
    "    bd1 = tf.Variable(tf.zeros(120))\n",
    "    \n",
    "    # fully connected layer\n",
    "    fc1 =  tf.reshape(conv2,[-1,wd1.get_shape().as_list()[0]])\n",
    "    fc1 =  tf.add(tf.matmul(fc1,wd1),bd1)\n",
    "    fc1 =  tf.nn.relu(fc1)\n",
    "    \n",
    "    wd2 = tf.Variable(tf.truncated_normal([120,84],mu=mu,stddev=sigma))\n",
    "    bd2 = tf.Variable(tf.zeros(84))\n",
    "\n",
    "    fc2 = tf.add(tf.matmul(fc1,wd2),bd2)\n",
    "    fc2 = tf.nn.relu(fc2)\n",
    "    \n",
    "    wd3 = tf.Variable(tf.truncated_normal([84,10],mu=mu,stddev=sigma))\n",
    "    bd3 = tf.Variable(tf.zeros(10))\n",
    "    \n",
    "    logits = tf.add(tf.matmul(fc2,wd3),bd3)\n",
    "    \n",
    "    return logits\n",
    "    \n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "    \n",
    "x = tf.placeholder(tf.float32,[None,784])    \n",
    "y = tf.placeholder(tf.float32,[None,10])\n",
    "\n",
    "fc2 = LeNet(x)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax.cross_entropy_with_logits(fc2))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(fc2,1),tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session as sess:\n",
    "    sess.run(init)\n",
    "    steps_per_epoch = mnist.train.num_examples//BATCH_SIZE\n",
    "    num_examples = steps_per_epoch*BATCH_SIZE\n",
    "    \n",
    "    for epoch_iter in range(EPOCHS)    \n",
    "        for step in range(steps_per_epoch):\n",
    "            batch_x, batch_y = mnist.train.next_batch(BATCH_SIZE)\n",
    "            sess.run(optimizer, feed_dict={x: batch_x, y:batch_y})\n",
    "         \n",
    "        steps_per_epoch = mnist.validation.num_examples // BATCH_SIZE\n",
    "        num_examples =  steps_per_epoch*BATCH_SIZE\n",
    "        for step in range(steps_per_epoch):\n",
    "            batch_x, batch_y = mnist.validation.next_batch(BATCH_SIZE)\n",
    "            loss = sess.run(cost, feed_dict= {x:batch_x, y:batch_y})\n",
    "            acc  = sess.run(accuracy,feed_dict={x: batch_x, y: batch_y})\n",
    "            total_acc += (acc*batch_x.shape[0])\n",
    "            total_loss += (loss*batch_x.shape[0])\n",
    "            \n",
    "        print(\"EPOCH {} ...\".format(i+1))\n",
    "        print(\"Validation loss = {:.3f}\".format(total_loss/num_examples))\n",
    "        print(\"Validation accuracy = {:.3f}\".format(total_acc/num_examples))\n",
    "        print()\n",
    "\n",
    "    # Evaluate on the test data\n",
    "    steps_per_epoch = mnist.test.num_examples//BATCH_SIZE\n",
    "    num_examples = steps_per_epoch*BATCH_SIZE\n",
    "    for step in range(steps_per_epoch):\n",
    "        batch_x, batch_y = mnist.test_next_batch(BATCH_SIZE)\n",
    "        test_loss = sess.run(cost,feed_dict={x: batch_x, y: batch_y})\n",
    "        test_acc  = sess.run(accuracy,feed_dict={x: batch_x, y: batch_y})\n",
    "        total_acc += test_acc*batch_x.shape[0]\n",
    "        total_loss += test_loss*batch_x.shape[0]\n",
    "        \n",
    "    print(\"Test loss = {:.3f}\".format(total_loss/num_examples))\n",
    "    print(\"Test accuracy = {:.3f}\".format(total_acc/num_examples))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
